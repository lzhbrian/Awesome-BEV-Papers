# BEV-Papers
curated list of BEV related papers. 

I also organized DETR related papers here, as they are also closely related to most recent papers.

I am intensely reading BEV related papers these days, so this list is expected to be updated very frequently.

### TOC
* [BEV 3D Object Detection](#bev-3d-object-detection-related)
* [BEV Segmentation](#bev-segmentation-related)
* [DETR Series](#detr-series)


### BEV 3D Object Detection related

- **[H-DETR]** DETRs with Hybrid Matching.<br>
Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, Han Hu.<br>
In . [[2207.13080](https://arxiv.org/abs/2207.13080)] [[HDETR/H-Deformable-DETR](https://github.com/HDETR/H-Deformable-DETR)]
- **[AutoAlignV2]** AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection.<br>
Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao.<br>
In . [[2207.10316](https://arxiv.org/abs/2207.10316)] [[zehuichen123/AutoAlignV2](https://github.com/zehuichen123/AutoAlignV2)]
- **[PolarFormer]** PolarFormer: Multi-camera 3D Object Detection with Polar Transformers.<br>
Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang.<br>
In . [[2206.15398](https://arxiv.org/abs/2206.15398)]
- **[SRCN3D]** SRCN3D: Sparse R-CNN 3D Surround-View Camera Object Detection and Tracking for Autonomous Driving.<br>
Yining Shi, Jingyan Shen, Yifan Sun, Yunlong Wang, Jiaxin Li, Shiqi Sun, Kun Jiang, Diange Yang.<br>
In . [[2206.14451](https://arxiv.org/abs/2206.14451)] [[synsin0/SRCN3D](https://github.com/synsin0/SRCN3D)]
- **[BEVDepth]** BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection.<br>
Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, Zeming Li.<br>
In . [[2206.10092](https://arxiv.org/abs/2206.10092)] [[Megvii-BaseDetection/BEVDepth](https://github.com/Megvii-BaseDetection/BEVDepth)]
- **[Ego3RT]** Learning Ego 3D Representation as Ray Tracing.<br>
Jiachen Lu, Zheyuan Zhou, Xiatian Zhu, Hang Xu, Li Zhang.<br>
In . [[2206.04042](https://arxiv.org/abs/2206.04042)] [[fudan-zvg/Ego3RT](https://github.com/fudan-zvg/Ego3RT)]
- **[PETRv2]** PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images.<br>
Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Qi Gao, Tiancai Wang, Xiangyu Zhang, Jian Sun.<br>
In . [[2206.01256](https://arxiv.org/abs/2206.01256)] [[megvii-research/PETR](https://github.com/megvii-research/PETR)]
- **[UVTR]** Unifying Voxel-based Representation with Transformer for 3D Object Detection<br>
Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, Jiaya Jia.<br>
In . [[2206.00630](https://arxiv.org/abs/2206.00630)] [[dvlab-research/UVTR](https://github.com/dvlab-research/UVTR)]
- **[BEVFusion2]** BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework<br>
Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, Zhi Tang.<br>
In . [[2205.13790](https://arxiv.org/abs/2205.13790)] [[ADLab-AutoDrive/BEVFusion](https://github.com/ADLab-AutoDrive/BEVFusion)]
- **[BEVFusion1]** BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation<br>
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, Song Han.<br>
In . [[2205.13542](https://arxiv.org/abs/2205.13542)] [[mit-han-lab/bevfusion](https://github.com/mit-han-lab/bevfusion)]
- **[BEVerse]** BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving.<br>
Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang, Guan Huang, Jie Zhou, Jiwen Lu.<br>
In . [[2205.09743](https://arxiv.org/abs/2205.09743)] [[zhangyp15/BEVerse](https://github.com/zhangyp15/BEVerse)]
- **[Graph-DETR3D]** Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection<br>
Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao.<br>
In . [[2204.11582](https://arxiv.org/abs/2204.11582)]
- **[M2BEV]** M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation<br>
Enze Xie, Zhiding Yu, Daquan Zhou, Jonah Philion, Anima Anandkumar, Sanja Fidler, Ping Luo, Jose M. Alvarez.<br>
In . [[2204.05088](https://arxiv.org/abs/2204.05088)] [[NVlabs/M2BEV](https://github.com/NVlabs/M2BEV)]
- **[BEVFormer]** BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers<br>
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, Jifeng Dai.<br>
In . [[2203.17270](https://arxiv.org/abs/2203.17270)] [[zhiqi-li/BEVFormer](https://github.com/zhiqi-li/BEVFormer)]
- **[BEVDet4D]** BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection.<br>
Junjie Huang, Guan Huang.<br>
In . [[2203.17054](https://arxiv.org/abs/2203.17054)] [[HuangJunJie2017/BEVDet](https://github.com/HuangJunJie2017/BEVDet)]
- **[PETR]** PETR: Position Embedding Transformation for Multi-View 3D Object Detection<br>
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, Jian Sun.<br>
In . [[2203.05625](https://arxiv.org/abs/2203.05625)] [[megvii-research/PETR](https://github.com/megvii-research/PETR)]
- **[BEVDet]** BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View.<br>
Junjie Huang, Guan Huang, Zheng Zhu, Dalong Du.<br>
In . [[2112.11790](https://arxiv.org/abs/2112.11790)] [[HuangJunJie2017/BEVDet](https://github.com/HuangJunJie2017/BEVDet)]
- **[DETR3D]** DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries.<br>
Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, Justin Solomon.<br>
In CoRL 2021. [[2110.06922](https://arxiv.org/abs/2110.06922)] [[wangyueft/detr3d](https://github.com/wangyueft/detr3d)]




### BEV Segmentation related
- **[UniFormer]** Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View.<br>
Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen, Xi Li.<br>
In . [[2207.08536](https://arxiv.org/abs/2207.08536)]
- **[CoBEVT]** CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers.<br>
Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei Zhou, Jiaqi Ma.<br>
In . [[2207.02202](https://arxiv.org/abs/2207.02202)]
- **[GKT]** Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer<br>
Shaoyu Chen, Tianheng Cheng, Xinggang Wang, Wenming Meng, Qian Zhang, Wenyu Liu.<br>
In . [[2206.04584](https://arxiv.org/abs/2206.04584)] [[hustvl/GKT](https://github.com/hustvl/GKT)]
- **[ViT-BEVSeg]** ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation.<br>
Pramit Dutta, Ganesh Sistu, Senthil Yogamani, Edgar Galván, John McDonald.<br>
In WCCI 2022. [[2205.15667](https://arxiv.org/abs/2205.15667)]
- **[Cross-view Transformers]** Cross-view Transformers for real-time Map-view Semantic Segmentation.<br>
Brady Zhou, Philipp Krähenbühl.<br>
In CVPR 2022. [[2205.02833](https://arxiv.org/abs/2205.02833)] [[bradyz/cross_view_transformers](https://github.com/bradyz/cross_view_transformers)]
- **[GitNet]** GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation.<br>
Shi Gong, Xiaoqing Ye, Xiao Tan, Jingdong Wang, Errui Ding, Yu Zhou, Xiang Bai.<br>
In . [[2204.07733](https://arxiv.org/abs/2204.07733)]
- **[HFT]** HFT: Lifting Perspective Representations via Hybrid Feature Transformation.<br>
Jiayu Zou, Junrui Xiao, Zheng Zhu, Junjie Huang, Guan Huang, Dalong Du, Xingang Wang.<br>
In . [[2204.05068](https://arxiv.org/abs/2204.05068)] [[JiayuZou2020/HFT](https://github.com/JiayuZou2020/HFT)]
- **[PersFormer]** PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark.<br>
Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan.<br>
In . [[2203.11089](https://arxiv.org/abs/2203.11089)] [[OpenPerceptionX/PersFormer_3DLane](https://github.com/OpenPerceptionX/PersFormer_3DLane)]
- **[BEVSegFormer]** BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs.<br>
Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang, Erkang Cheng.<br>
In . [[2203.04050](https://arxiv.org/abs/2203.04050)]
- **[STSU]** Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images.<br>
Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool<br>
In ICCV 2021. [[2110.01997](https://arxiv.org/abs/2110.01997)] [[ybarancan/STSU](https://github.com/ybarancan/STSU)]
- **[TIM]** Translating Images into Maps.<br>
Avishkar Saha, Oscar Mendez Maldonado, Chris Russell, Richard Bowden<br>
In ICRA 2022. [[2110.00966](https://arxiv.org/abs/2110.00966)] [[avishkarsaha/translating-images-into-maps](https://github.com/avishkarsaha/translating-images-into-maps)]
- **[NEAT]** NEAT: Neural Attention Fields for End-to-End Autonomous Driving.<br>
Kashyap Chitta, Aditya Prakash, Andreas Geiger.<br>
In ICCV 2021. [[2109.04456](https://arxiv.org/abs/2109.04456)]
- **[BEV Panoptic]** Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images<br>
Nikhil Gosala, Abhinav Valada.<br>
In RA-L 2021. [[2108.03227](https://arxiv.org/abs/2108.03227)] [[code](http://rl.uni-freiburg.de/research/panoptic-bev)]
- **[Disentangling and Vectorization]** Disentangling and Vectorization: A 3D Visual Perception Approach for Autonomous Driving Based on Surround-View Fisheye Cameras.<br>
Zizhang Wu, Wenkai Zhang, Jizheng Wang, Man Wang, Yuanzhu Gan, Xinchao Gou, Muqing Fang, Jing Song.<br>
In IROS 2021. [[2107.08862](https://arxiv.org/abs/2107.08862)]
- **[HDMapNet]** HDMapNet: An Online HD Map Construction and Evaluation Framework<br>
Qi Li, Yue Wang, Yilun Wang, Hang Zhao.<br>
In ICRA 2022. [[2107.06307](https://arxiv.org/abs/2107.06307)] [[Tsinghua-MARS-Lab/HDMapNet](https://github.com/Tsinghua-MARS-Lab/HDMapNet)]
- **[FIERY]** FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras<br>
Anthony Hu, Zak Murez, Nikhil Mohan, Sofia Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, Alex Kendall.<br>
In ICCV 2021. [[2104.10490](https://arxiv.org/abs/2104.10490)] [[wayveai/fiery](https://github.com/wayveai/fiery)]
- **[STA]** Enabling spatio-temporal aggregation in Birds-Eye-View Vehicle Estimation<br>
Avishkar Saha, Oscar Mendez, Chris Russell, Richard Bowden.
In ICRA 2021. [[paper](https://cvssp.org/Personal/OscarMendez/papers/pdf/SahaICRA2021.pdf)]
- **[EPOSH]** Bird’s Eye View Segmentation Using Lifted 2D Semantic Features.<br>
Isht Dwivedi, Srikanth Malla, Yi-Ting Chen, Behzad Dariush.<br>
In BMVC 2021. [[paper](https://www.bmvc2021-virtualconference.com/assets/papers/0772.pdf)]
- **[PYVA]** Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-view Transformation.<br>
Weixiang Yang, Qi Li, Wenxi Liu, Yuanlong Yu, Yuexin Ma, Shengfeng He, Jia Pan.<br>
In CVPR 2021. [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.html)] [[JonDoe-297/cross-view](https://github.com/JonDoe-297/cross-view)]
- **[BEV feat stitch]** Understanding Bird's-Eye View of Road Semantics using an Onboard Camera<br>
Yigit Baran Can, Alexander Liniger, Ozan Unal, Danda Paudel, Luc Van Gool.<br>
In RA-L 2021. [[2012.03040](https://arxiv.org/abs/2012.03040)] [[ybarancan/BEV_feat_stitch](https://github.com/ybarancan/BEV_feat_stitch)]
- **[LSS]** Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D.<br>
Jonah Philion, Sanja Fidler.<br>
In ECCV 2020. [[2008.05711](https://arxiv.org/abs/2008.05711)] [[nv-tlabs/lift-splat-shoot](https://github.com/nv-tlabs/lift-splat-shoot)]
- **[BEV-Seg]** BEV-Seg: Bird's Eye View Semantic Segmentation Using Geometry and Semantic Point Cloud<br>
Mong H. Ng, Kaahan Radia, Jianfei Chen, Dequan Wang, Ionel Gog, Joseph E. Gonzalez.<br>
In CVPRW 2020. [[2006.11436](https://arxiv.org/abs/2006.11436)]
- **[Cam2BEV]** A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View.<br>
Lennart Reiher, Bastian Lampe, Lutz Eckstein.<br>
In ITSC 2020. [[2005.04078](https://arxiv.org/abs/2005.04078)] [[ika-rwth-aachen/Cam2BEV](https://github.com/ika-rwth-aachen/Cam2BEV)]
- **[PyrOccNet]** Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks<br>
Thomas Roddick, Roberto Cipolla.<br>
In . [[2003.13402](https://arxiv.org/abs/2003.13402)] [[tom-roddick/mono-semantic-maps](https://github.com/tom-roddick/mono-semantic-maps)]
- **[MonoLayout]** MonoLayout: Amodal scene layout from a single image<br>
Kaustubh Mani, Swapnil Daga, Shubhika Garg, N. Sai Shankar, Krishna Murthy Jatavallabhula, K. Madhava Krishna.<br>
In WACV 2020. [[2002.08394](https://arxiv.org/abs/2002.08394)] [[hbutsuak95/monolayout](https://github.com/hbutsuak95/monolayout)]
- **[VPN]** Cross-view Semantic Segmentation for Sensing Surroundings<br>
Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, Bolei Zhou.<br>
In RA-L 2020. [[1906.03560](https://arxiv.org/abs/1906.03560)] [[pbw-Berwin/View-Parsing-Network](https://github.com/pbw-Berwin/View-Parsing-Network)]
- **[OFT]** Orthographic Feature Transform for Monocular 3D Object Detection<br>
Thomas Roddick, Alex Kendall, Roberto Cipolla.<br>
In BMVC 2019. [[1811.08188](https://arxiv.org/abs/1811.08188)] [[tom-roddick/oft](https://github.com/tom-roddick/oft)]
- **[VED]** Monocular Semantic Occupancy Grid Mapping with Convolutional Variational Encoder-Decoder Networks.<br>
Chenyang Lu, Marinus Jacobus Gerardus van de Molengraft, Gijs Dubbelman.<br>
In RA-L 2019. [[1804.02176](https://arxiv.org/abs/1804.02176)]
- Learning to Look around Objects for Top-View Representations of Outdoor Scenes<br>
Samuel Schulter, Menghua Zhai, Nathan Jacobs, Manmohan Chandraker.<br>
In ECCV 2018. [[1803.10870](https://arxiv.org/abs/1803.10870)]
- **[MapNet]** MapNet: An Allocentric Spatial Memory for Mapping Environments.<br>
Joao F. Henriques Andrea Vedaldi.<br>
In CVPR 2018. [[paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Henriques_MapNet_An_Allocentric_CVPR_2018_paper.pdf)]
- **[Mapping]** Automatic Dense Visual Semantic Mapping from Street-Level Imagery.<br>
Sunando Sengupta, Paul Sturgess, L’ubor Ladický, Philip H. S. Torr.<br>
In IROS 2012. [[paper](https://www.robots.ox.ac.uk/~tvg/publications/2012/IROS_Mapping_ss.pdf)]


### DETR Series
- **[DETR++]** DETR++: Taming Your Multi-Scale Detection Transformer.<br>
Chi Zhang, Lijuan Liu, Xiaoxue Zang, Frederick Liu, Hao Zhang, Xinying Song, Jindong Chen.<br>
In CVPRW 2022. [[2206.02977](https://arxiv.org/abs/2206.02977)]
- **[Mask DINO]** Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation.<br>
Feng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, Heung-Yeung Shum.<br>
In . [[2206.02777](https://arxiv.org/abs/2206.02777)] [[IDEACVR/MaskDINO](https://github.com/IDEACVR/MaskDINO)]
- **[DDQ]** What Are Expected Queries in End-to-End Object Detection?<br>
Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Kai Chen.<br>
In . [[2206.01232](https://arxiv.org/abs/2206.01232)] [[jshilong/DDQ](https://github.com/jshilong/DDQ)]
- **[Dynamic Sparse R-CNN]** Dynamic Sparse R-CNN.<br>
Qinghang Hong, Fengming Liu, Dong Li, Ji Liu, Lu Tian, Yi Shan.<br>
In CVPR 2022. [[2205.02101](https://arxiv.org/abs/2205.02101)]
- **[DINO]** DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection<br>
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum.<br>
In . [[2203.03605](https://arxiv.org/abs/2203.03605)]
- **[DN-DETR]** DN-DETR: Accelerate DETR Training by Introducing Query DeNoising.<br>
Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang.<br>
In CVPR 2022. [[2203.01305](https://arxiv.org/abs/2203.01305)] [[IDEA-opensource/DN-DETR](https://github.com/IDEA-opensource/DN-DETR)]
- **[D^2ETR]** D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention.<br>
Junyu Lin, Xiaofeng Mao, Yuefeng Chen, Lei Xu, Yuan He, Hui Xue.<br>
In . [[2203.00860](https://arxiv.org/abs/2203.00860)]
- **[DAB-DETR]** DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR.<br>
Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.<br>
In ICLR 2022. [[2201.12329](https://arxiv.org/abs/2201.12329)] [[IDEA-opensource/DAB-DETR](https://github.com/IDEA-opensource/DAB-DETR)]
- **[Deformable Attention]** Vision Transformer with Deformable Attention.<br>
Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, Gao Huang.<br>
In CVPR 2022. [[2201.00520](https://arxiv.org/abs/2201.00520)] [[LeapLabTHU/DAT](https://github.com/LeapLabTHU/DAT)]
- **[Sparse DETR]** Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity<br>
Byungseok Roh, JaeWoong Shin, Wuhyun Shin, Saehoon Kim.<br>
In ICLR 2022. [[2111.14330](https://arxiv.org/abs/2111.14330)] [[kakaobrain/sparse-detr](https://github.com/kakaobrain/sparse-detr)]
- **[Anchor DETR]** Anchor DETR: Query Design for Transformer-Based Object Detection.<br>
Yingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun.<br>
In AAAI 2022. [[2109.07107](https://arxiv.org/abs/2109.07107)] [[megvii-research/AnchorDETR](https://github.com/megvii-research/AnchorDETR)]
- **[Dynamic DETR]** Dynamic DETR: End-to-End Object Detection With Dynamic Attention.<br>
Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, Lei Zhang.<br>
In ICCV 2021. [[paper](https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.html?ref=https://githubhelp.com)]
- **[Conditional DETR]** Conditional DETR for Fast Training Convergence<br>
Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.<br>
In ICCV 2021. [[2108.06152](https://arxiv.org/abs/2108.06152)] [[Atten4Vis/ConditionalDETR](https://github.com/Atten4Vis/ConditionalDETR)]
- **[Efficient DETR]** Efficient DETR: Improving End-to-End Object Detector with Dense Prior<br>
Zhuyu Yao, Jiangbo Ai, Boxun Li, Chi Zhang.<br>
In . [[2104.01318](https://arxiv.org/abs/2104.01318)]
- **[SMCA]** Fast Convergence of DETR with Spatially Modulated Co-Attention<br>
Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, Hongsheng Li.<br>
In . [[2101.07448](https://arxiv.org/abs/2101.07448)] [[gaopengcuhk/SMCA-DETR](https://github.com/gaopengcuhk/SMCA-DETR)]
- **[Sparse R-CNN]** Sparse R-CNN: End-to-End Object Detection with Learnable Proposals<br>
Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, Ping Luo.<br>
In . [[2011.12450](https://arxiv.org/abs/2011.12450)] [[PeizeSun/SparseR-CNN](https://github.com/PeizeSun/SparseR-CNN)]
- **[TSP]** Rethinking Transformer-based Set Prediction for Object Detection<br>
Zhiqing Sun, Shengcao Cao, Yiming Yang, Kris Kitani.<br>
In ICCV 2021. [[2011.10881](https://arxiv.org/abs/2011.10881)]
- **[Deformable DETR]** Deformable DETR: Deformable Transformers for End-to-End Object Detection.<br>
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.<br>
In ICLR 2021. [[2010.04159](https://arxiv.org/abs/2010.04159)] [[fundamentalvision/Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR)]
- **[DETR]** End-to-End Object Detection with Transformers.<br>
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.<br>
In [[2005.12872](https://arxiv.org/abs/2005.12872)] [[facebookresearch/detr](https://github.com/facebookresearch/detr)]






